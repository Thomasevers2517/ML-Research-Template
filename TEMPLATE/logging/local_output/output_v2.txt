wandb: Starting wandb agent üïµÔ∏è
2024-11-25 11:43:11,050 - wandb.wandb_agent - INFO - Running runs: []
2024-11-25 11:43:11,310 - wandb.wandb_agent - INFO - Agent received command: run
2024-11-25 11:43:11,311 - wandb.wandb_agent - INFO - Agent starting run with config:
	DATA_LOADER: Image_Dataloader
	DATA_LOADER_PARAMS: {'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}
	LOSS_FN: CrossEntropyLoss
	MODEL: VIT
	MODEL_PARAMS: {'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.001}
	OPTIMIZER: AdamW
	OPTIMIZER_PARAMS: {'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}
	TEST_RUN: False
	TRAINER: BaseTrainer
	TRAINER_PARAMS: {'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}
2024-11-25 11:43:11,317 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python TEMPLATE/main.py --DATA_LOADER=Image_Dataloader "--DATA_LOADER_PARAMS={'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}" --LOSS_FN=CrossEntropyLoss --MODEL=VIT "--MODEL_PARAMS={'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.001}" --OPTIMIZER=AdamW "--OPTIMIZER_PARAMS={'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}" --TEST_RUN=False --TRAINER=BaseTrainer "--TRAINER_PARAMS={'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}"
2024-11-25 11:43:16,334 - wandb.wandb_agent - INFO - Running runs: ['2b3qf628']
wandb: Currently logged in as: thomasevers9. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Path TEMPLATE/log/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path TEMPLATE/log/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /tmp/wandb/run-20241125_114318-2b3qf628
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE
wandb: üßπ View sweep at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/sweeps/4uy1rsku
wandb: üöÄ View run at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/runs/2b3qf628
CONFIGURATION 
 {'TEST_RUN': False, 'MODEL': 'VIT', 'MODEL_PARAMS': {'NUM_LAYERS': 8, 'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.001}, 'OPTIMIZER': 'AdamW', 'OPTIMIZER_PARAMS': {'LR': 0.0001, 'NUM_EPOCHS': 250, 'LR_SCHEDULER': 'None'}, 'LOSS_FN': 'CrossEntropyLoss', 'TRAINER': 'BaseTrainer', 'TRAINER_PARAMS': {'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1, 'DATA_PARALLEL': [False], 'DEVICES': [0]}, 'DATA_LOADER': 'Image_Dataloader', 'DATA_LOADER_PARAMS': {'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data', 'BATCH_SIZE': 8}, 'WANDB_LOGGING_PARAMS': {'PROJECT': 'template_test', 'DIR': 'TEMPLATE/log'}, 'EARLY_STOPPING_PARAMS': {'PATIENCE': 10, 'DELTA': 0.001, 'VERBOSE': False}}
Traceback (most recent call last):
  File "/users/thomasevers/users/thomas/ML_Research_Template/ML-Research-Template/TEMPLATE/main.py", line 34, in <module>
    print(f"Using devices: {TRAIN_CONFIG['TRAINER_PARAMS']['DEVICES']} and DATA_PARALLEL: {DATA_PARALLEL}")
                                                                                           ^^^^^^^^^^^^^
NameError: name 'DATA_PARALLEL' is not defined
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb: | 0.027 MB of 0.027 MB uploadedwandb: / 0.027 MB of 0.037 MB uploadedwandb:                                                                                
wandb: üöÄ View run flowing-sweep-1 at: https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/runs/2b3qf628
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20241125_114318-2b3qf628/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2024-11-25 11:43:31,767 - wandb.wandb_agent - INFO - Cleaning up finished run: 2b3qf628
2024-11-25 11:43:32,124 - wandb.wandb_agent - INFO - Agent received command: run
2024-11-25 11:43:32,124 - wandb.wandb_agent - INFO - Agent starting run with config:
	DATA_LOADER: Image_Dataloader
	DATA_LOADER_PARAMS: {'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}
	LOSS_FN: CrossEntropyLoss
	MODEL: VIT
	MODEL_PARAMS: {'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.0001}
	OPTIMIZER: AdamW
	OPTIMIZER_PARAMS: {'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}
	TEST_RUN: False
	TRAINER: BaseTrainer
	TRAINER_PARAMS: {'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}
2024-11-25 11:43:32,128 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python TEMPLATE/main.py --DATA_LOADER=Image_Dataloader "--DATA_LOADER_PARAMS={'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}" --LOSS_FN=CrossEntropyLoss --MODEL=VIT "--MODEL_PARAMS={'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.0001}" --OPTIMIZER=AdamW "--OPTIMIZER_PARAMS={'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}" --TEST_RUN=False --TRAINER=BaseTrainer "--TRAINER_PARAMS={'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}"
2024-11-25 11:43:37,141 - wandb.wandb_agent - INFO - Running runs: ['h3mi9r08']
wandb: Currently logged in as: thomasevers9. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Path TEMPLATE/log/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path TEMPLATE/log/wandb/ wasn't writable, using system temp directory
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in /tmp/wandb/run-20241125_114337-h3mi9r08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE
wandb: üßπ View sweep at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/sweeps/4uy1rsku
wandb: üöÄ View run at https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/runs/h3mi9r08
CONFIGURATION 
 {'TEST_RUN': False, 'MODEL': 'VIT', 'MODEL_PARAMS': {'NUM_LAYERS': 8, 'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0.0001}, 'OPTIMIZER': 'AdamW', 'OPTIMIZER_PARAMS': {'LR': 0.0001, 'NUM_EPOCHS': 250, 'LR_SCHEDULER': 'None'}, 'LOSS_FN': 'CrossEntropyLoss', 'TRAINER': 'BaseTrainer', 'TRAINER_PARAMS': {'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1, 'DATA_PARALLEL': [False], 'DEVICES': [0]}, 'DATA_LOADER': 'Image_Dataloader', 'DATA_LOADER_PARAMS': {'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data', 'BATCH_SIZE': 8}, 'WANDB_LOGGING_PARAMS': {'PROJECT': 'template_test', 'DIR': 'TEMPLATE/log'}, 'EARLY_STOPPING_PARAMS': {'PATIENCE': 10, 'DELTA': 0.001, 'VERBOSE': False}}
Traceback (most recent call last):
  File "/users/thomasevers/users/thomas/ML_Research_Template/ML-Research-Template/TEMPLATE/main.py", line 34, in <module>
    print(f"Using devices: {TRAIN_CONFIG['TRAINER_PARAMS']['DEVICES']} and DATA_PARALLEL: {DATA_PARALLEL}")
                                                                                           ^^^^^^^^^^^^^
NameError: name 'DATA_PARALLEL' is not defined
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb: | 0.037 MB of 0.037 MB uploadedwandb:                                                                                
wandb: üöÄ View run mild-sweep-2 at: https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE/runs/h3mi9r08
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/thomasevers9/ML-Research-Template-TEMPLATE
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/wandb/run-20241125_114337-h3mi9r08/logs
wandb: WARNING The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core
2024-11-25 11:43:52,535 - wandb.wandb_agent - INFO - Cleaning up finished run: h3mi9r08
2024-11-25 11:43:52,841 - wandb.wandb_agent - INFO - Agent received command: run
2024-11-25 11:43:52,842 - wandb.wandb_agent - INFO - Agent starting run with config:
	DATA_LOADER: Image_Dataloader
	DATA_LOADER_PARAMS: {'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}
	LOSS_FN: CrossEntropyLoss
	MODEL: VIT
	MODEL_PARAMS: {'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0}
	OPTIMIZER: AdamW
	OPTIMIZER_PARAMS: {'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}
	TEST_RUN: False
	TRAINER: BaseTrainer
	TRAINER_PARAMS: {'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}
2024-11-25 11:43:52,845 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python TEMPLATE/main.py --DATA_LOADER=Image_Dataloader "--DATA_LOADER_PARAMS={'BATCH_SIZE': 8, 'DATASET_NAME': 'CIFAR100', 'DATA_DIR': 'data'}" --LOSS_FN=CrossEntropyLoss --MODEL=VIT "--MODEL_PARAMS={'EMBEDDING_SIZE': 256, 'NUM_HEADS': 4, 'NUM_LAYERS': 8, 'PATCH_SIZE': 1, 'T_THRESHOLD': 0}" --OPTIMIZER=AdamW "--OPTIMIZER_PARAMS={'LR': 0.0001, 'LR_SCHEDULER': 'None', 'NUM_EPOCHS': 250}" --TEST_RUN=False --TRAINER=BaseTrainer "--TRAINER_PARAMS={'DEVICES': [0], 'LOG_INTERVAL': 20, 'VAL_INTERVAL': 1}"
wandb: Ctrl-c pressed. Waiting for runs to end. Press ctrl-c again to terminate them.
wandb: Terminating and syncing runs. Press ctrl-c to kill.
wandb: Killing runs and quitting.
